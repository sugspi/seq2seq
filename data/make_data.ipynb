{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from __future__ import print_function\n",
    "from gensim import corpora\n",
    "\n",
    "from nltk.sem.logic import *\n",
    "from nltk.sem.logic import LogicParser\n",
    "\n",
    "\n",
    "\n",
    "##日本語のため##\n",
    "import MeCab\n",
    "mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-58ef386d49a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "#SNLIからテキスト部分取り出し\n",
    "#ここで文書がユニークかどうかを見ている\n",
    "#そのため，ペアにしたい場合は注意が必要\n",
    "\n",
    "lines = []\n",
    "with open('') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        lines.append(line)\n",
    "        line = f.readline()\n",
    "txt = open('jp_full0319.txt', 'w') # 書き込みモードで開く\n",
    "\n",
    "#count= 0;\n",
    "texts_set = set()\n",
    "for i in lines:\n",
    "    #count +=1\n",
    "    dic  = json.loads(i)\n",
    "    s1 = dic['sentence1']\n",
    "    s2 = dic['sentence2']\n",
    "    \n",
    "    #origin data set\n",
    "    #txt.write(s1+ \"\\n\")\n",
    "    #txt.write(s2+ \"\\n\")\n",
    "    \n",
    "    \n",
    "    #uniq data set\n",
    "    if   s1 not in texts_set:\n",
    "        txt.write(s1+ \"\\n\") \n",
    "        texts_set.add(s1)\n",
    "    if  s2 not in texts_set:\n",
    "        txt.write(s2+ \"\\n\")\n",
    "        texts_set.add(s2)\n",
    " \n",
    "#    if(100<count):\n",
    "#        break\n",
    "    \n",
    "txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#京大含意関係認識用\n",
    "\n",
    "# input\n",
    "\n",
    "#kana\n",
    "lines = []\n",
    "with open('data/ku_data.txt') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        lst = line.split(\" \")\n",
    "        \n",
    "        #lst[3]= lst[3].replace('\\r','')\n",
    "        #lst[3] = lst[3].replace('。','。\\n')\n",
    "        \n",
    "        lst[4] = lst[4].replace('\\n','')\n",
    "        lst[4] = lst[4].replace('\\r','')\n",
    "        lst[4] = lst[4].replace('。','。\\n')\n",
    "\n",
    "        lines.append(lst[3]+\"\\n\")\n",
    "        lines.append(lst[4])        \n",
    "        line = f.readline()\n",
    "        \n",
    "#print(len(lines)/2)\n",
    "\n",
    "txt = open('test.txt', 'w') # 書き込みモードで開く\n",
    "\n",
    "\n",
    "count= 0;\n",
    "for i in lines:\n",
    "    count +=1\n",
    "    #print(i)\n",
    "    txt.write(i) \n",
    "    #if(100<count):\n",
    "    #    break\n",
    "    \n",
    "txt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "9934\n"
     ]
    }
   ],
   "source": [
    "#変数を統一するための処理\n",
    "#ラムダを取っている\n",
    "\n",
    "\n",
    "pre_var = []\n",
    "formulas = []\n",
    "check_dup = set([])\n",
    "\n",
    "#from nltk.sem.logic import LogicParser\n",
    "#from nltk.sem.logic import *\n",
    "\n",
    "logic_parser = LogicParser(type_check=False)\n",
    "def lexpr(formula_str):\n",
    "    return logic_parser.parse(formula_str)\n",
    "\n",
    "def coq_string_expr(expression):\n",
    "    \n",
    "    org_exp = expression\n",
    "    \n",
    "    if isinstance(expression, str):\n",
    "        expression = lexpr(expression)\n",
    "    expr_coq_str = ''\n",
    "    if isinstance(expression, ApplicationExpression):\n",
    "        coq_string_application_expr(expression)\n",
    "    elif isinstance(expression, AbstractVariableExpression):\n",
    "        expr_coq_str = coq_string_abstract_variable_expr(expression)\n",
    "    elif isinstance(expression, LambdaExpression):\n",
    "        raise\n",
    "    elif isinstance(expression, QuantifiedExpression):\n",
    "        expr_coq_str = coq_string_quantified_expr(expression)\n",
    "    elif isinstance(expression, AndExpression):\n",
    "        expr_coq_str = coq_string_and_expr(expression)\n",
    "    elif isinstance(expression, OrExpression):\n",
    "        expr_coq_str = coq_string_or_expr(expression)\n",
    "    elif isinstance(expression, NegatedExpression):\n",
    "        expr_coq_str = coq_string_not_expr(expression)\n",
    "    elif isinstance(expression, BinaryExpression):\n",
    "        expr_coq_str = coq_string_binary_expr(expression)\n",
    "    elif isinstance(expression, Variable):\n",
    "        expr_coq_str = '%s' % expression\n",
    "    else:\n",
    "        expr_coq_str = str(expression)\n",
    "\n",
    "    return org_exp\n",
    "\n",
    "coqstr = coq_string_expr\n",
    "\n",
    "def coq_string_application_expr(expression):\n",
    "    # uncurry the arguments and find the base function\n",
    "    if expression.is_atom():\n",
    "        function, args = expression.uncurry()\n",
    "        arg_str = ' '.join(\"%s\" % coqstr(arg) for arg in args)\n",
    "    else:\n",
    "        #Leave arguments curried\n",
    "        function = expression.function\n",
    "        arg_str = \"%s\" % coqstr(expression.argument)\n",
    "    function_str = \"%s\" % coqstr(function)\n",
    "    parenthesize_function = False\n",
    "    if isinstance(function, LambdaExpression):\n",
    "        if isinstance(function.term, ApplicationExpression):\n",
    "            if not isinstance(function.term.function,\n",
    "                              AbstractVariableExpression):\n",
    "                parenthesize_function = True\n",
    "        elif not isinstance(function.term, BooleanExpression):\n",
    "            parenthesize_function = True\n",
    "    elif isinstance(function, ApplicationExpression):\n",
    "        parenthesize_function = True\n",
    "    if parenthesize_function:\n",
    "        function_str = Tokens.OPEN + function_str + Tokens.CLOSE\n",
    "    return expression \n",
    "\n",
    "def coq_string_abstract_variable_expr(expression):\n",
    "    expr_str = str(expression.variable)\n",
    "    if not isinstance(expression, FunctionVariableExpression):\n",
    "        if expr_str == '':\n",
    "            expr_str = \"%s\" % expr_str\n",
    "        else:\n",
    "            expr_str = \"%s\" % expr_str\n",
    "    else:\n",
    "        expr_str = \"%s\" % expr_str\n",
    "        \n",
    "    if not re.sub(r'_', \"\",expr_str) in check_dup:\n",
    "        if not expr_str.startswith('_'):\n",
    "            pre_var.append(expr_str)\n",
    "            check_dup.add(expr_str)\n",
    "    return expression\n",
    "\n",
    "\n",
    "def coq_string_quantified_expr(expression):\n",
    "    variables = [expression.variable]\n",
    "    term = expression.term\n",
    "    while term.__class__ == expression.__class__:\n",
    "        variables.append(term.variable)\n",
    "        term = term.term\n",
    "    for v in variables:\n",
    "        coqstr(v)\n",
    "    coqstr(term) \n",
    "    return expression\n",
    "\n",
    "def coq_string_and_expr(expression):\n",
    "    first = coqstr(expression.first)\n",
    "    second = coqstr(expression.second)\n",
    "    return expression \n",
    "\n",
    "def coq_string_or_expr(expression):\n",
    "    first = coqstr(expression.first)\n",
    "    second = coqstr(expression.second)\n",
    "    return expression\n",
    "\n",
    "def coq_string_not_expr(expression):\n",
    "    term_str = coqstr(expression.term)\n",
    "    return expression\n",
    "\n",
    "def coq_string_binary_expr(expression):\n",
    "    first = coqstr(expression.first)\n",
    "    second = coqstr(expression.second)\n",
    "    return expression\n",
    "\n",
    "\n",
    "def substituteString(text,lst):\n",
    "    ###辞書作り###\n",
    "    lst = list(set(lst))\n",
    "    e_sub = {}\n",
    "    z_sub = {}\n",
    "    ice = 0 #index counter\n",
    "    icz = 0 \n",
    "\n",
    "    for i in lst:\n",
    "        matchObj = re.search(r'e0+', i)\n",
    "        if(matchObj):\n",
    "            tmp = \"e0\"+str(ice)\n",
    "            e_sub[tmp] = i\n",
    "            ice+=1\n",
    "            continue\n",
    "        matchObj = re.search(r'z[0-9]*', i)\n",
    "        if matchObj:\n",
    "            tmp = \"z0\"+str(icz)\n",
    "            z_sub[tmp] = i\n",
    "            icz+=1\n",
    "    \n",
    "    e_sub = {v:k for k, v in e_sub.items()}\n",
    "    z_sub = {v:k for k, v in z_sub.items()}\n",
    "    \n",
    "    for k, v in e_sub.items():  \n",
    "        text = text.replace(k,v)\n",
    "    for k, v in z_sub.items():  \n",
    "        text = text.replace(k,v)\n",
    "    return text\n",
    "\n",
    "\n",
    "tree = ET.parse('/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aa.candc.sem.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "root = root[0]\n",
    "root = root[0] #１個目のsentence\n",
    "\n",
    "mydict = {}\n",
    "mydict2 = {}\n",
    "\n",
    "#c = 0 #辞書のindexを回す\n",
    "print(len(root))\n",
    "\n",
    "#子階層のタグと中身\n",
    "for c,child in enumerate(root):\n",
    "    \n",
    "    formula = child[2]  #child[2]がsemantics\n",
    "    check = (child[2].attrib)\n",
    "    check = check['status']\n",
    "    \n",
    "   \n",
    "    if(check == 'success'):\n",
    "        plain = \"\"\n",
    "        base = \"\"\n",
    "        toridashi = child[0]\n",
    "        \n",
    "        for i in toridashi :\n",
    "            att = i.attrib\n",
    "            p = att['surf']\n",
    "            b = att['base']\n",
    "            if(p=='.' or p== ','):\n",
    "                plain = plain + p\n",
    "                base = base + b\n",
    "            else :\n",
    "                plain = plain + \" \" + p\n",
    "                base = base + \" \" + b\n",
    "        plain = plain + '\\n'\n",
    "        base = base + '\\n'\n",
    "        \n",
    "        formula = child[2][0]\n",
    "        formula = (formula.attrib)\n",
    "        formula = formula['sem'] #\\nが付与されていると信じている\n",
    "        \n",
    "        #論理式を今回の標準にする\n",
    "        check_dup = set([])\n",
    "        pre_var = []\n",
    "        \n",
    "        try:\n",
    "            org_f = formula\n",
    "            formula = coq_string_expr(formula)\n",
    "        except:\n",
    "            #import traceback\n",
    "            #traceback.print_exc()\n",
    "            continue\n",
    "            \n",
    "        if(formula == 'lambda_expression'):\n",
    "            continue\n",
    "        formula = substituteString(formula,pre_var)\n",
    "        ice = 0\n",
    "        icz = 0\n",
    "\n",
    "        for i in range(len(pre_var)):\n",
    "            matchObj = re.search(r'e0+', pre_var[i])\n",
    "            if(matchObj):\n",
    "                tmp = \"e0\"+str(ice)\n",
    "                pre_var[i] = tmp\n",
    "\n",
    "            matchObj = re.search(r'z[0-9]*', pre_var[i])\n",
    "            if matchObj:\n",
    "                tmp = \"z0\"+str(icz)\n",
    "                pre_var[i] = tmp\n",
    "                icz+=1\n",
    "\n",
    "        pair = {'text':plain,'formula':formula}\n",
    "        pair2 = {'text':plain,'formula':formula,'base':base}\n",
    "        mydict.update({str(c):pair})\n",
    "        mydict2.update({str(c):pair2})\n",
    "            \n",
    "    else:\n",
    "      print('faild parse')\n",
    "    #c+=1\n",
    "    #if(c==100):\n",
    "    #    break\n",
    "print(len(mydict))\n",
    "    \n",
    "f = open('snli_full/eng_ap.json', 'w') # 書き込みモードで開く\n",
    "json.dump(mydict, f,ensure_ascii=False)\n",
    "f.close()\n",
    "\n",
    "f = open('1.txt', 'w') # 書き込みモードで開く\n",
    "json.dump(mydict2, f,ensure_ascii=False)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
