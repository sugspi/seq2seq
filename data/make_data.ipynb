{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "\n",
    "from __future__ import print_function\n",
    "from gensim import corpora\n",
    "\n",
    "from nltk.sem.logic import *\n",
    "from nltk.sem.logic import LogicParser\n",
    "\n",
    "\n",
    "\n",
    "##日本語のため##\n",
    "import MeCab\n",
    "mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-58ef386d49a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "#SNLIからテキスト部分取り出し\n",
    "#ここで文書がユニークかどうかを見ている\n",
    "#そのため，ペアにしたい場合は注意が必要\n",
    "\n",
    "lines = []\n",
    "with open('') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        lines.append(line)\n",
    "        line = f.readline()\n",
    "txt = open('jp_full0319.txt', 'w') # 書き込みモードで開く\n",
    "\n",
    "#count= 0;\n",
    "texts_set = set()\n",
    "for i in lines:\n",
    "    #count +=1\n",
    "    dic  = json.loads(i)\n",
    "    s1 = dic['sentence1']\n",
    "    s2 = dic['sentence2']\n",
    "    \n",
    "    #origin data set\n",
    "    #txt.write(s1+ \"\\n\")\n",
    "    #txt.write(s2+ \"\\n\")\n",
    "    \n",
    "    \n",
    "    #uniq data set\n",
    "    if   s1 not in texts_set:\n",
    "        txt.write(s1+ \"\\n\") \n",
    "        texts_set.add(s1)\n",
    "    if  s2 not in texts_set:\n",
    "        txt.write(s2+ \"\\n\")\n",
    "        texts_set.add(s2)\n",
    " \n",
    "#    if(100<count):\n",
    "#        break\n",
    "    \n",
    "txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#京大含意関係認識用\n",
    "\n",
    "# input\n",
    "\n",
    "#kana\n",
    "lines = []\n",
    "with open('data/ku_data.txt') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        lst = line.split(\" \")\n",
    "        \n",
    "        #lst[3]= lst[3].replace('\\r','')\n",
    "        #lst[3] = lst[3].replace('。','。\\n')\n",
    "        \n",
    "        lst[4] = lst[4].replace('\\n','')\n",
    "        lst[4] = lst[4].replace('\\r','')\n",
    "        lst[4] = lst[4].replace('。','。\\n')\n",
    "\n",
    "        lines.append(lst[3]+\"\\n\")\n",
    "        lines.append(lst[4])        \n",
    "        line = f.readline()\n",
    "        \n",
    "#print(len(lines)/2)\n",
    "\n",
    "txt = open('test.txt', 'w') # 書き込みモードで開く\n",
    "\n",
    "\n",
    "count= 0;\n",
    "for i in lines:\n",
    "    count +=1\n",
    "    #print(i)\n",
    "    txt.write(i) \n",
    "    #if(100<count):\n",
    "    #    break\n",
    "    \n",
    "txt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aa.candc.sem.xml\n",
      "10000\n",
      "9934\n",
      "ab\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ab.candc.sem.xml\n",
      "9999\n",
      "9901\n",
      "ac\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ac.candc.sem.xml\n",
      "9994\n",
      "faild parse\n",
      "9806\n",
      "ad\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ad.candc.sem.xml\n",
      "9987\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9969\n",
      "ae\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ae.candc.sem.xml\n",
      "9987\n",
      "faild parse\n",
      "9455\n",
      "af\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-af.candc.sem.xml\n",
      "9993\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9968\n",
      "ag\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ag.candc.sem.xml\n",
      "9990\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9808\n",
      "ah\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ah.candc.sem.xml\n",
      "9993\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9910\n",
      "ai\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ai.candc.sem.xml\n",
      "9995\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9886\n",
      "aj\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aj.candc.sem.xml\n",
      "9987\n",
      "faild parse\n",
      "9884\n",
      "ak\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ak.candc.sem.xml\n",
      "9992\n",
      "faild parse\n",
      "9929\n",
      "al\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-al.candc.sem.xml\n",
      "9981\n",
      "faild parse\n",
      "faild parse\n",
      "9976\n",
      "am\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-am.candc.sem.xml\n",
      "9990\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9967\n",
      "an\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-an.candc.sem.xml\n",
      "9999\n",
      "9992\n",
      "ao\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ao.candc.sem.xml\n",
      "9981\n",
      "faild parse\n",
      "faild parse\n",
      "9967\n",
      "ap\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ap.candc.sem.xml\n",
      "9984\n",
      "faild parse\n",
      "9974\n",
      "aq\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aq.candc.sem.xml\n",
      "9995\n",
      "faild parse\n",
      "faild parse\n",
      "9975\n",
      "ar\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ar.candc.sem.xml\n",
      "9990\n",
      "faild parse\n",
      "9975\n",
      "as\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-as.candc.sem.xml\n",
      "9995\n",
      "9991\n",
      "at\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-at.candc.sem.xml\n",
      "9937\n",
      "faild parse\n",
      "9918\n",
      "au\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-au.candc.sem.xml\n",
      "9984\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9969\n",
      "av\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-av.candc.sem.xml\n",
      "9986\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9971\n",
      "aw\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aw.candc.sem.xml\n",
      "9991\n",
      "9978\n",
      "ax\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ax.candc.sem.xml\n",
      "9933\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9919\n",
      "ay\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ay.candc.sem.xml\n",
      "9995\n",
      "faild parse\n",
      "9902\n",
      "az\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-az.candc.sem.xml\n",
      "9994\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9840\n",
      "ba\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ba.candc.sem.xml\n",
      "9991\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9919\n",
      "bb\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bb.candc.sem.xml\n",
      "9989\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9632\n",
      "bc\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bc.candc.sem.xml\n",
      "9992\n",
      "faild parse\n",
      "9739\n",
      "bd\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bd.candc.sem.xml\n",
      "9982\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9754\n",
      "be\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-be.candc.sem.xml\n",
      "9974\n",
      "faild parse\n",
      "9947\n",
      "bf\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bf.candc.sem.xml\n",
      "9990\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9971\n",
      "bg\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bg.candc.sem.xml\n",
      "9994\n",
      "9992\n",
      "bh\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bh.candc.sem.xml\n",
      "9982\n",
      "faild parse\n",
      "faild parse\n",
      "9968\n",
      "bi\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bi.candc.sem.xml\n",
      "9987\n",
      "faild parse\n",
      "9974\n",
      "bj\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bj.candc.sem.xml\n",
      "9996\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9981\n",
      "bk\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bk.candc.sem.xml\n",
      "9976\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9956\n",
      "bl\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bl.candc.sem.xml\n",
      "9964\n",
      "faild parse\n",
      "faild parse\n",
      "9957\n",
      "bm\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bm.candc.sem.xml\n",
      "9990\n",
      "9986\n",
      "bn\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bn.candc.sem.xml\n",
      "9989\n",
      "faild parse\n",
      "faild parse\n",
      "9982\n",
      "bo\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bo.candc.sem.xml\n",
      "9980\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9968\n",
      "bp\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bp.candc.sem.xml\n",
      "9979\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9966\n",
      "bq\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bq.candc.sem.xml\n",
      "9995\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "faild parse\n",
      "9978\n",
      "br\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-br.candc.sem.xml\n",
      "9992\n",
      "faild parse\n",
      "9983\n",
      "bs\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bs.candc.sem.xml\n",
      "9991\n",
      "9984\n",
      "bt\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bt.candc.sem.xml\n",
      "9983\n",
      "faild parse\n",
      "faild parse\n",
      "9960\n",
      "bu\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bu.candc.sem.xml\n",
      "9986\n",
      "faild parse\n",
      "faild parse\n",
      "9975\n",
      "bv\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bv.candc.sem.xml\n",
      "9980\n",
      "faild parse\n",
      "faild parse\n",
      "9955\n",
      "bw\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bw.candc.sem.xml\n",
      "42\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "#変数を統一するための処理\n",
    "#ラムダを取っている\n",
    "\n",
    "\n",
    "pre_var = []\n",
    "formulas = []\n",
    "check_dup = set([])\n",
    "\n",
    "#from nltk.sem.logic import LogicParser\n",
    "#from nltk.sem.logic import *\n",
    "\n",
    "logic_parser = LogicParser(type_check=False)\n",
    "def lexpr(formula_str):\n",
    "    return logic_parser.parse(formula_str)\n",
    "\n",
    "def coq_string_expr(expression):\n",
    "    \n",
    "    org_exp = expression\n",
    "    \n",
    "    if isinstance(expression, str):\n",
    "        expression = lexpr(expression)\n",
    "    expr_coq_str = ''\n",
    "    if isinstance(expression, ApplicationExpression):\n",
    "        coq_string_application_expr(expression)\n",
    "    elif isinstance(expression, AbstractVariableExpression):\n",
    "        expr_coq_str = coq_string_abstract_variable_expr(expression)\n",
    "    elif isinstance(expression, LambdaExpression):\n",
    "        raise\n",
    "    elif isinstance(expression, QuantifiedExpression):\n",
    "        expr_coq_str = coq_string_quantified_expr(expression)\n",
    "    elif isinstance(expression, AndExpression):\n",
    "        expr_coq_str = coq_string_and_expr(expression)\n",
    "    elif isinstance(expression, OrExpression):\n",
    "        expr_coq_str = coq_string_or_expr(expression)\n",
    "    elif isinstance(expression, NegatedExpression):\n",
    "        expr_coq_str = coq_string_not_expr(expression)\n",
    "    elif isinstance(expression, BinaryExpression):\n",
    "        expr_coq_str = coq_string_binary_expr(expression)\n",
    "    elif isinstance(expression, Variable):\n",
    "        expr_coq_str = '%s' % expression\n",
    "    else:\n",
    "        expr_coq_str = str(expression)\n",
    "\n",
    "    return org_exp\n",
    "\n",
    "coqstr = coq_string_expr\n",
    "\n",
    "def coq_string_application_expr(expression):\n",
    "    # uncurry the arguments and find the base function\n",
    "    if expression.is_atom():\n",
    "        function, args = expression.uncurry()\n",
    "        arg_str = ' '.join(\"%s\" % coqstr(arg) for arg in args)\n",
    "    else:\n",
    "        #Leave arguments curried\n",
    "        function = expression.function\n",
    "        arg_str = \"%s\" % coqstr(expression.argument)\n",
    "    function_str = \"%s\" % coqstr(function)\n",
    "    parenthesize_function = False\n",
    "    if isinstance(function, LambdaExpression):\n",
    "        if isinstance(function.term, ApplicationExpression):\n",
    "            if not isinstance(function.term.function,\n",
    "                              AbstractVariableExpression):\n",
    "                parenthesize_function = True\n",
    "        elif not isinstance(function.term, BooleanExpression):\n",
    "            parenthesize_function = True\n",
    "    elif isinstance(function, ApplicationExpression):\n",
    "        parenthesize_function = True\n",
    "    if parenthesize_function:\n",
    "        function_str = Tokens.OPEN + function_str + Tokens.CLOSE\n",
    "    return expression \n",
    "\n",
    "def coq_string_abstract_variable_expr(expression):\n",
    "    expr_str = str(expression.variable)\n",
    "    if not isinstance(expression, FunctionVariableExpression):\n",
    "        if expr_str == '':\n",
    "            expr_str = \"%s\" % expr_str\n",
    "        else:\n",
    "            expr_str = \"%s\" % expr_str\n",
    "    else:\n",
    "        expr_str = \"%s\" % expr_str\n",
    "        \n",
    "    if not re.sub(r'_', \"\",expr_str) in check_dup:\n",
    "        if not expr_str.startswith('_'):\n",
    "            pre_var.append(expr_str)\n",
    "            check_dup.add(expr_str)\n",
    "    return expression\n",
    "\n",
    "\n",
    "def coq_string_quantified_expr(expression):\n",
    "    variables = [expression.variable]\n",
    "    term = expression.term\n",
    "    while term.__class__ == expression.__class__:\n",
    "        variables.append(term.variable)\n",
    "        term = term.term\n",
    "    for v in variables:\n",
    "        coqstr(v)\n",
    "    coqstr(term) \n",
    "    return expression\n",
    "\n",
    "def coq_string_and_expr(expression):\n",
    "    first = coqstr(expression.first)\n",
    "    second = coqstr(expression.second)\n",
    "    return expression \n",
    "\n",
    "def coq_string_or_expr(expression):\n",
    "    first = coqstr(expression.first)\n",
    "    second = coqstr(expression.second)\n",
    "    return expression\n",
    "\n",
    "def coq_string_not_expr(expression):\n",
    "    term_str = coqstr(expression.term)\n",
    "    return expression\n",
    "\n",
    "def coq_string_binary_expr(expression):\n",
    "    first = coqstr(expression.first)\n",
    "    second = coqstr(expression.second)\n",
    "    return expression\n",
    "\n",
    "\n",
    "def substituteString(text,lst):\n",
    "    ###辞書作り###\n",
    "    lst = list(set(lst))\n",
    "    e_sub = {}\n",
    "    z_sub = {}\n",
    "    ice = 0 #index counter\n",
    "    icz = 0 \n",
    "\n",
    "    for i in lst:\n",
    "        matchObj = re.search(r'e0+', i)\n",
    "        if(matchObj):\n",
    "            tmp = \"e0\"+str(ice)\n",
    "            e_sub[tmp] = i\n",
    "            ice+=1\n",
    "            continue\n",
    "        matchObj = re.search(r'z[0-9]*', i)\n",
    "        if matchObj:\n",
    "            tmp = \"z0\"+str(icz)\n",
    "            z_sub[tmp] = i\n",
    "            icz+=1\n",
    "    \n",
    "    e_sub = {v:k for k, v in e_sub.items()}\n",
    "    z_sub = {v:k for k, v in z_sub.items()}\n",
    "    \n",
    "    for k, v in e_sub.items():  \n",
    "        text = text.replace(k,v)\n",
    "    for k, v in z_sub.items():  \n",
    "        text = text.replace(k,v)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "a1 = 'a'\n",
    "a2 = 'a'\n",
    "\n",
    "for fname in glob.glob('/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-*.candc.sem.xml'):\n",
    "    print(a1+a2)\n",
    "    print (fname)\n",
    "    \n",
    "    pre_var = []\n",
    "    formulas = []\n",
    "    check_dup = set([])\n",
    "\n",
    "    tree = ET.parse(fname)#('/Users/guru/Downloads/snli_train_data/parsed/text/snli-train-t-aa.candc.sem.xml')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    root = root[0]\n",
    "    root = root[0] #１個目のsentence\n",
    "\n",
    "    mydict = {}\n",
    "    mydict2 = {}\n",
    "\n",
    "    #c = 0 #辞書のindexを回す\n",
    "    print(len(root))\n",
    "\n",
    "    #子階層のタグと中身\n",
    "    for c,child in enumerate(root):\n",
    "\n",
    "        formula = child[2]  #child[2]がsemantics\n",
    "        check = (child[2].attrib)\n",
    "        check = check['status']\n",
    "\n",
    "\n",
    "        if(check == 'success'):\n",
    "            plain = \"\"\n",
    "            base = \"\"\n",
    "            toridashi = child[0]\n",
    "\n",
    "            for i in toridashi :\n",
    "                att = i.attrib\n",
    "                p = att['surf']\n",
    "                b = att['base']\n",
    "                if(p=='.' or p== ','):\n",
    "                    plain = plain + p\n",
    "                    base = base + b\n",
    "                else :\n",
    "                    plain = plain + \" \" + p\n",
    "                    base = base + \" \" + b\n",
    "            plain = plain + '\\n'\n",
    "            base = base + '\\n'\n",
    "\n",
    "            formula = child[2][0]\n",
    "            formula = (formula.attrib)\n",
    "            formula = formula['sem'] #\\nが付与されていると信じている\n",
    "\n",
    "            #論理式を今回の標準にする\n",
    "            check_dup = set([])\n",
    "            pre_var = []\n",
    "\n",
    "            try:\n",
    "                org_f = formula\n",
    "                formula = coq_string_expr(formula)\n",
    "            except:\n",
    "                #import traceback\n",
    "                #traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "            if(formula == 'lambda_expression'):\n",
    "                continue\n",
    "            formula = substituteString(formula,pre_var)\n",
    "            ice = 0\n",
    "            icz = 0\n",
    "\n",
    "            for i in range(len(pre_var)):\n",
    "                matchObj = re.search(r'e0+', pre_var[i])\n",
    "                if(matchObj):\n",
    "                    tmp = \"e0\"+str(ice)\n",
    "                    pre_var[i] = tmp\n",
    "\n",
    "                matchObj = re.search(r'z[0-9]*', pre_var[i])\n",
    "                if matchObj:\n",
    "                    tmp = \"z0\"+str(icz)\n",
    "                    pre_var[i] = tmp\n",
    "                    icz+=1\n",
    "\n",
    "            pair = {'text':plain,'formula':formula}\n",
    "            pair2 = {'text':plain,'formula':formula,'base':base}\n",
    "            mydict.update({str(c):pair})\n",
    "            mydict2.update({str(c):pair2})\n",
    "\n",
    "        else:\n",
    "          print('faild parse')\n",
    "        #c+=1\n",
    "        #if(c==100):\n",
    "        #    break\n",
    "    print(len(mydict))\n",
    "\n",
    "    f = open('snli_full/eng_aa.json', 'w') # 書き込みモードで開く\n",
    "    json.dump(mydict, f,ensure_ascii=False)\n",
    "    f.close()\n",
    "\n",
    "    f = open('tmp/h/'+a1+a2+'.txt', 'w') # 書き込みモードで開く\n",
    "    json.dump(mydict2, f,ensure_ascii=False)\n",
    "    f.close()\n",
    "        \n",
    "    a2 = chr(ord(a2) + 1)\n",
    "    if(ord(a2)==123):\n",
    "        a2 = 'a'\n",
    "        a1 = chr(ord(a1) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "97\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aa.candc.sem.xml\n",
      "ab\n",
      "98\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ab.candc.sem.xml\n",
      "ac\n",
      "99\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ac.candc.sem.xml\n",
      "ad\n",
      "100\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ad.candc.sem.xml\n",
      "ae\n",
      "101\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ae.candc.sem.xml\n",
      "af\n",
      "102\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-af.candc.sem.xml\n",
      "ag\n",
      "103\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ag.candc.sem.xml\n",
      "ah\n",
      "104\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ah.candc.sem.xml\n",
      "ai\n",
      "105\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ai.candc.sem.xml\n",
      "aj\n",
      "106\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aj.candc.sem.xml\n",
      "ak\n",
      "107\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ak.candc.sem.xml\n",
      "al\n",
      "108\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-al.candc.sem.xml\n",
      "am\n",
      "109\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-am.candc.sem.xml\n",
      "an\n",
      "110\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-an.candc.sem.xml\n",
      "ao\n",
      "111\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ao.candc.sem.xml\n",
      "ap\n",
      "112\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ap.candc.sem.xml\n",
      "aq\n",
      "113\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aq.candc.sem.xml\n",
      "ar\n",
      "114\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ar.candc.sem.xml\n",
      "as\n",
      "115\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-as.candc.sem.xml\n",
      "at\n",
      "116\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-at.candc.sem.xml\n",
      "au\n",
      "117\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-au.candc.sem.xml\n",
      "av\n",
      "118\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-av.candc.sem.xml\n",
      "aw\n",
      "119\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-aw.candc.sem.xml\n",
      "ax\n",
      "120\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ax.candc.sem.xml\n",
      "ay\n",
      "121\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ay.candc.sem.xml\n",
      "az\n",
      "122\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-az.candc.sem.xml\n",
      "ba\n",
      "97\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-ba.candc.sem.xml\n",
      "bb\n",
      "98\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bb.candc.sem.xml\n",
      "bc\n",
      "99\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bc.candc.sem.xml\n",
      "bd\n",
      "100\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bd.candc.sem.xml\n",
      "be\n",
      "101\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-be.candc.sem.xml\n",
      "bf\n",
      "102\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bf.candc.sem.xml\n",
      "bg\n",
      "103\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bg.candc.sem.xml\n",
      "bh\n",
      "104\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bh.candc.sem.xml\n",
      "bi\n",
      "105\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bi.candc.sem.xml\n",
      "bj\n",
      "106\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bj.candc.sem.xml\n",
      "bk\n",
      "107\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bk.candc.sem.xml\n",
      "bl\n",
      "108\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bl.candc.sem.xml\n",
      "bm\n",
      "109\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bm.candc.sem.xml\n",
      "bn\n",
      "110\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bn.candc.sem.xml\n",
      "bo\n",
      "111\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bo.candc.sem.xml\n",
      "bp\n",
      "112\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bp.candc.sem.xml\n",
      "bq\n",
      "113\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bq.candc.sem.xml\n",
      "br\n",
      "114\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-br.candc.sem.xml\n",
      "bs\n",
      "115\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bs.candc.sem.xml\n",
      "bt\n",
      "116\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bt.candc.sem.xml\n",
      "bu\n",
      "117\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bu.candc.sem.xml\n",
      "bv\n",
      "118\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bv.candc.sem.xml\n",
      "bw\n",
      "119\n",
      "/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-bw.candc.sem.xml\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "a = 'a'\n",
    "b = 'a'\n",
    "\n",
    "\n",
    "for fname in glob.glob('/Users/guru/Downloads/snli_train_data/parsed/hypothesis/snli-train-h-*.candc.sem.xml'):\n",
    "    print(a+b)\n",
    "    print (fname)\n",
    "    \n",
    "    b = chr(ord(b) + 1)\n",
    "    if(ord(b)==123):\n",
    "        b = 'a'\n",
    "        a = chr(ord(a) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
