{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers          import Lambda, Input, Dense, GRU, LSTM, RepeatVector\n",
    "from keras.models          import Model\n",
    "from keras.layers.core     import Flatten\n",
    "from keras.callbacks       import LambdaCallback\n",
    "from keras.optimizers      import SGD, RMSprop, Adam\n",
    "from keras.layers.wrappers import Bidirectional as Bi\n",
    "from keras.layers.wrappers import TimeDistributed as TD\n",
    "from keras.layers          import merge, multiply\n",
    "from keras.regularizers    import l2\n",
    "from keras.layers.core     import Reshape\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import MeCab\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timesteps   = 50\n",
    "inputs      = Input(shape=(timesteps, 128))\n",
    "encoded     = LSTM(512)(inputs)\n",
    "inputs_a    = inputs\n",
    "inputs_a    = Dense(2048)(inputs_a)\n",
    "inputs_a    = BN()(inputs_a)\n",
    "a_vector    = Dense(512, activation='softmax')(Flatten()(inputs_a))\n",
    "mul         = multiply([encoded, a_vector])\n",
    "encoder     = Model(inputs, mul)\n",
    "\n",
    "x           = RepeatVector(timesteps)(mul)\n",
    "x           = Bi(LSTM(512, return_sequences=True))(x)\n",
    "decoded     = TD(Dense(128, activation='softmax'))(x)\n",
    "\n",
    "autoencoder = Model(inputs, decoded)\n",
    "autoencoder.compile(optimizer=Adam(), loss='categorical_crossentropy')\n",
    "\n",
    "buff = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callbacks(epoch, logs):\n",
    "  global buff\n",
    "  buff = copy.copy(logs)\n",
    "  print(\"epoch\" ,epoch)\n",
    "  print(\"logs\", logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  c_i = pickle.loads( open(\"dict_pkl/c_i_formula.pkl\", \"rb\").read() )\n",
    "  c_i2 = pickle.loads( open(\"dict_pkl/c_i_text.pkl\", \"rb\").read() )\n",
    "  xss = []\n",
    "  yss = []\n",
    "\n",
    "  f =  open(\"ku_teach.txt\", \"r\") \n",
    "  lines = [line for line in f]\n",
    "  #print( len(lines) )\n",
    "  random.shuffle( lines )\n",
    "  for fi, line in enumerate(lines):\n",
    "    if fi >= 15000:\n",
    "      break\n",
    "    line = line.strip()\n",
    "    head, tail = line.split(\"___SP___\")\n",
    "    if(len(head) > 50 or len(tail) > 50):\n",
    "        continue\n",
    "\n",
    "    xs = [ [0.]*128 for _ in range(50) ]\n",
    "    ys = [ [0.]*128 for _ in range(50) ]\n",
    "    \n",
    "    for i, c in enumerate(head):\n",
    "       if(c_i.get(c)):\n",
    "         xs[i][c_i[c]] = 1.\n",
    "    \n",
    "    \n",
    "    for i, c in enumerate(tail):\n",
    "       if(c_i2.get(c)):\n",
    "         ys[i][c_i2[c]] = 1.\n",
    "    \n",
    "    xss.append( np.array( list(reversed(xs)) ) )\n",
    "    yss.append( np.array( list(reversed(ys)) ) )\n",
    "    \n",
    "  f.close()\n",
    "        \n",
    "  Xs = np.array( xss )\n",
    "  Ys = np.array( yss )\n",
    "  print(Xs.shape)\n",
    "    \n",
    "\n",
    "    #for i in range(2000):\n",
    "  print_callback = LambdaCallback(on_epoch_end=callbacks)\n",
    "  batch_size = random.randint( 32, 64 )\n",
    "  random_optim = random.choice( [Adam(), SGD(), RMSprop()] )\n",
    "  print( random_optim )\n",
    "  autoencoder.optimizer = random_optim\n",
    "\n",
    "\n",
    "  #traing\n",
    "  keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "  earlystop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=10, verbose=0, mode='auto')\n",
    "  tensorboard = keras.callbacks.TensorBoard(log_dir='logs', histogram_freq=0, write_graph=True)\n",
    "\n",
    "  autoencoder.fit( Xs, Ys,  shuffle=True, batch_size=batch_size, epochs=100)\n",
    "  now = str(time.time())\n",
    "\n",
    "  autoencoder.save('model/'+ 'ku_epoc100' + '.h5')\n",
    "  import gc; gc.collect()\n",
    "  print(\"saved ..\")\n",
    "  print(\"logs...\", buff )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(429, 50, 128)\n",
      "<keras.optimizers.SGD object at 0x1400b3c88>\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 35s - loss: 1.0047    \n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 31s - loss: 1.0045    \n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 33s - loss: 1.0043    \n",
      "Epoch 4/100\n",
      "106/429 [======>.......................] - ETA: 24s - loss: 0.9798"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "  c_i = pickle.loads( open(\"dict_pkl/c_i_formula.pkl\", \"rb\").read() )\n",
    "  c_i2 = pickle.loads( open(\"dict_pkl/c_i_text.pkl\", \"rb\").read() )\n",
    "  i_c = { i:c for c, i in c_i.items() }\n",
    "  i_c2 = { i:c for c, i in c_i2.items() }\n",
    "  xss = []\n",
    "  heads = []\n",
    "  with open(\"testinp.txt\", \"r\") as f: \n",
    "    for fi, line in enumerate(f):\n",
    "      head = line.strip()\n",
    "      #head, tail = line.split(\"___SP___\")\n",
    "      if(len(head)>50):\n",
    "        break\n",
    "      print(head)\n",
    "      heads.append( head )\n",
    "    \n",
    "      xs = [ [0.]*128 for _ in range(50) ]\n",
    "      for i, c in enumerate(head):\n",
    "        if(c_i.get(c)):\n",
    "         xs[i][c_i[c]] = 1.\n",
    "      xss.append( np.array( list(reversed(xs)) ) )\n",
    "\n",
    "  Xs = np.array( xss )\n",
    "  print( Xs)\n",
    "  #model =open(\"model/ku_first.h5\")\n",
    "  #print(\"loaded model is \", model)\n",
    "  autoencoder.load_weights(\"model/ku_first.h5\")\n",
    "\n",
    "  Ys = autoencoder.predict( Xs ).tolist()\n",
    "  for ez, (head, y) in enumerate(zip(heads, Ys)):\n",
    "    terms = []\n",
    "    for v in y:\n",
    "      term = max( [(s, i_c2[i]) for i,s in enumerate(v)] , key=lambda x:x[0])[1]\n",
    "      terms.append( term )\n",
    "    tail = re.sub(r\"」.*?$\", \"」\", \"\".join( terms ) )\n",
    "    print(ez, head, \"___SP___\", tail )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existsv.(_ギョウテン(v)&_スル(v)&Past(v))\n",
      "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "0 existsv.(_ギョウテン(v)&_スル(v)&Past(v)) ___SP___ 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
